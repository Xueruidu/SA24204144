---
title: "Summary of Homework"
author: "Xuerui Du"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Summary of Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


## Homework-2024.09.09


### Question

1. Go through "R for Beginners" if you are not familiar with R programming.

2. Use knitr to produce at least 3 examples. For each example, texts should mix with figures and/or tables. Better to have mathematical formulas. 


### Answer

1. I'm familiar with R programming.


2. 

1)Assuming that ${X}$  obeys the standard normal distribution $\mathcal{N}(0,1)$ and ${Y}$ obeys the distribution, then the distribution $\chi^2(n)$.Then ${Z}=\frac{{X}}{\sqrt{Y/n}}$ is called the $t$-distribution with degrees of freedom $n$ and is denoted as $Z \sim t(n)$.



2)We toss the coin ten times repeatedly.Here is the result.

|No.|Result|
|:-:|:-:|
|1|H|
|2|T|
|3|H|
|4|T|
|5|T|
|6|H|
|7|T|
|8|H|
|9|H|
|10|T|




3)The general formula for the probability density function of the normal distribution is
$$
f(x) = \frac{1} {\sqrt{2\pi}\sigma} e^{-(x - \mu)^{2}/(2\sigma^{2}) }.
$$
The following is the plot of the standard normal probability density function.


## Homework-2024.09.14


### Question

1. The Rayleigh density [156, Ch. 18] is
$$
f(x) = \frac{x}{\sigma^2} e^{−x^2/(2\sigma^2)} \qquad x ≥ 0,σ> 0.
$$
Develop an algorithm to generate random samples from a Rayleigh($σ$) distribution. Generate Rayleigh($σ$) samples for several choices of $σ > 0$ and check that the mode of the generated samples is close to the theoretical mode $σ$(check the histogram).


2. Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have $\mathcal{N}(0, 1)$ and $\mathcal{N}(3, 1)$ distributions with mixing probabilities $p_1$ and $p_2 = 1 − p_1$. Graph the histogram of the sample with density superimposed, for $p_1 = 0.75$. Repeat with different values for $p_1$ and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of $p_1$ that produce bimodal mixtures.


3. $A \, compound \,  Poisson \, process$ is a stochastic process $\{X(t), t ≥ 0\}$ that can be represented as the random sum $X(t) = \sum^{N(t)}_{i=1} Y_i, t ≥ 0$, where $\{N(t), t ≥ 0\}$ is a Poisson process and $Y_1, Y_2,\dots$ are iid and independent of $\{N(t), t ≥ 0\}$.Write a program to simulate a compound Poisson($λ$)–Gamma process ($Y$ has a Gamma distribution). Estimate the mean and the variance of $X(10)$ for
several choices of the parameters and compare with the theoretical values.

Hint: Show that $E[X(t)] = λtE[Y_1]$ and $Var(X(t)) = λtE[Y_1^2]$.




### Answer

1.  To generate samples from the Rayleigh distribution using the inverse transform method, we first need to calculate its corresponding cumulative distribution function (CDF).

$$
\begin{aligned}
F(x) &= \int_0^x f(x)dx \\
&=\int_0^x \frac{x}{\sigma^2} e^{−x^2/(2\sigma^2)} dx \\
&= 1-e^{−x^2/(2\sigma^2)}
\end{aligned}
$$

then, 
$$
\begin{aligned}
u&=F(x)\\
u&=1-e^{−x^2/(2\sigma^2)} \\
e^{−x^2/(2\sigma^2)}&=1-u \\
x^2 &= -2\sigma^2 \cdot \log (1-u) \\
x&= \sqrt{ -2\sigma^2 \cdot \log (1-u) }
\end{aligned}
$$

Then, according to the inverse transform method:

a. Generate a variable $U \sim U(0,1)$;

b. Obtain $X=F_X^{-1}(U)$.

Here, the values of $\sigma$ are set to $\{ 1,10,50,100,1000 \}$, and the sample size is $1000$.


```{r,eval=FALSE}
invF <- function(u,sigma){
  x <- sqrt(  -2*sigma^2 * log(1-u) )
  return(x)
}
N <- 1000
set.seed(1234)
U <- runif(N,0,1)
X <- rep(0,N)
```


when $\sigma=1$
```{r,eval=FALSE}
sigma <- 1
X <- invF(U,sigma)
hist(X,probability = TRUE,main = expression(frac(x, sigma^2) * e^{-x^2 / (2 * sigma^2)},sigma==1),breaks = 10,ylim=c(0,0.65))
y <- seq(0,4,0.01)
lines(y,y/sigma^2*exp(-y^2/(2*sigma^2)),col="red")
```

when $\sigma=10$
```{r,eval=FALSE}
sigma <- 10
X <- invF(U,sigma)
hist(X,probability = TRUE,main = expression(frac(x, sigma^2) * e^{-x^2 / (2 * sigma^2)},sigma==10),breaks = 10,ylim=c(0,0.07))
y <- seq(0,40,0.01)
lines(y,y/sigma^2*exp(-y^2/(2*sigma^2)),col="red")
```

when $\sigma=50$
```{r,eval=FALSE}
sigma <- 50
X <- invF(U,sigma)
hist(X,probability = TRUE,main = expression(frac(x, sigma^2) * e^{-x^2 / (2 * sigma^2)},sigma==50),breaks = 10,ylim=c(0,0.013))
y <- seq(0,200,0.01)
lines(y,y/sigma^2*exp(-y^2/(2*sigma^2)),col="red")
```

when $\sigma=1000$
```{r,eval=FALSE}
sigma <- 1000
X <- invF(U,sigma)
hist(X,probability = TRUE,main = expression(frac(x, sigma^2) * e^{-x^2 / (2 * sigma^2)},sigma=1000),breaks = 10,ylim=c(0,0.0006))
y <- seq(0,4000,0.01)
lines(y,y/sigma^2*exp(-y^2/(2*sigma^2)),col="red")
```




2. For a normal distribution $\mathcal{N}(\mu,\sigma^2)$, its probability density function is
$$
f(x;\mu,\sigma^2) = \frac{1}{\sqrt{2 \pi}\sigma} \exp (- \frac{(x-\mu)^2}{2\sigma^2}).
$$

In this problem, the probability density function is
$$
\begin{aligned}
f(x;p_1,p_2) &= f(x|under \, model1)p(model1) + f(x|under \, model2)p(model2) \\
&= p_1 \cdot \frac{1}{\sqrt{2 \pi}} \exp (- \frac{x^2}{2}) + p_2 \cdot \frac{1}{\sqrt{2 \pi}} \exp (- \frac{(x-3)^2}{2}).
\end{aligned}
$$
when $p_1=0.75$
```{r,eval=FALSE}
N <- 1000
set.seed(1234)
X1 <- rnorm(N,0,1)
X2 <- rnorm(N,3,1)
p1 <- 0.75
p2 <- 1-p1
mixf <- function(x,p1){
  return( p1 / sqrt(2*pi) * exp(- x^2/2) + (1-p1) / sqrt(2*pi) * exp (- (x-3)^2/2) )
}
set.seed(123)
p <- sample(c(0,1),N,replace = TRUE,prob = c(p1,p2))
X <- p*X1 + p*X2
hist(X,breaks = 50,probability = TRUE,main = paste0("p1=",p1),xlim = c(-2,5))
y <- seq(-1,4,0.01)
lines(y,mixf(y,p1),col="red")
```

when $p_1 \in \{ 0,0.125,0.25,0.375,0.50,0.625,0.75,0.875,1 \}$
```{r,eval=FALSE}
for(p1 in (0:8)/8){
  p2 <- 1-p1
  set.seed(123)
  p <- sample(c(0,1),N,replace = TRUE,prob = c(p1,p2))
  X <- p*X1 + p*X2
  hist(X,breaks = 50,probability = TRUE,main = paste0("p1=",p1),xlim = c(-2,5))
  lines(y,mixf(y,p1),col="red")
}
```


The figures show that when $p_1$ is around $0.25$ to $0.75$, a bimodal phenomenon exists. The closer $p_1$ is to $0.5$, the more pronounced the bimodal phenomenon becomes.

3. It is known that $Y_1,Y_2,\dots$ are iid and independent of $\{N(t),t \geq 0\}$. $Y$ has a Gamma distribution and $\{N(t),t \geq 0\}$ is a Poisson($\lambda$) process, so
$$
\begin{aligned}
E[X(t)]&=E[E[X(t)|N(t)=n]] \\
&=E[nE[Y_1]] \\
&=E[Y_1] E[n] \\
&=E[Y_1] E[N(t)] \\
&=\lambda t E[Y_1]
\end{aligned}
$$

$$
\begin{aligned}
Var(X(t)) &= E[Var(X(t)|N(t)=n)] + Var(E[X(t)|N(t)=n]) \\
&= E[nVar(Y_1)] + Var(nE[Y_1]) \\
&= E[N(t)]Var(Y_1) + E[Y_1]^2Var(N(t)) \\
&= \lambda t (E[Y_1^2]-E[Y_1]^2) + E[Y_1]^2 \lambda t \\
&= \lambda t E[Y_1^2]
\end{aligned}
$$

We assume $Y_i\sim Gamma(\alpha,\beta)$,

```{r,eval=FALSE}
total <- 1000
t <- 10
lambda <- 1
alpha <- 1
beta <- 0.5
X10 <- numeric(total)
for(time in 1:total){
  set.seed(1234*time)
  N <- rpois(1,lambda*t)
  Y <- rgamma(N,shape = alpha,scale = beta)
  X10[time] <- sum(Y)
}
print(paste0("when lambda=",lambda,",alpha=",alpha,",beta=",beta))
print(paste0("the theoretical values:E[X(10)]=",lambda*t*alpha*beta,",Var(x(10))=",lambda*t*(alpha^2*beta^2+alpha*beta^2),";"))
print(paste0("the empirical values:E[X(10)]=",mean(X10),",Var(x(10))=",var(X10),";"))
```


```{r,eval=FALSE}
total <- 1000
t <- 10
lambda <- 1
alpha <- 2
beta <- 0.8
X10 <- numeric(total)
for(time in 1:total){
  set.seed(1234*time)
  N <- rpois(1,lambda*t)
  Y <- rgamma(N,shape = alpha,scale = beta)
  X10[time] <- sum(Y)
}
print(paste0("when lambda=",lambda,",alpha=",alpha,",beta=",beta))
print(paste0("the theoretical values:E[X(10)]=",lambda*t*alpha*beta,",Var(x(10))=",lambda*t*(alpha^2*beta^2+alpha*beta^2),";"))
print(paste0("the empirical values:E[X(10)]=",mean(X10),",Var(x(10))=",var(X10),";"))
```


```{r,eval=FALSE}
total <- 1000
t <- 10
lambda <- 1
alpha <- 5
beta <- 2
X10 <- numeric(total)
for(time in 1:total){
  set.seed(1234*time)
  N <- rpois(1,lambda*t)
  Y <- rgamma(N,shape = alpha,scale = beta)
  X10[time] <- sum(Y)
}
print(paste0("when lambda=",lambda,",alpha=",alpha,",beta=",beta))
print(paste0("the theoretical values:E[X(10)]=",lambda*t*alpha*beta,",Var(x(10))=",lambda*t*(alpha^2*beta^2+alpha*beta^2),";"))
print(paste0("the empirical values:E[X(10)]=",mean(X10),",Var(x(10))=",var(X10),";"))
```


```{r,eval=FALSE}
total <- 1000
t <- 10
lambda <- 3
alpha <- 1
beta <- 0.5
X10 <- numeric(total)
for(time in 1:total){
  set.seed(1234*time)
  N <- rpois(1,lambda*t)
  Y <- rgamma(N,shape = alpha,scale = beta)
  X10[time] <- sum(Y)
}
print(paste0("when lambda=",lambda,",alpha=",alpha,",beta=",beta))
print(paste0("the theoretical values:E[X(10)]=",lambda*t*alpha*beta,",Var(x(10))=",lambda*t*(alpha^2*beta^2+alpha*beta^2),";"))
print(paste0("the empirical values:E[X(10)]=",mean(X10),",Var(x(10))=",var(X10),";"))
```


```{r,eval=FALSE}
total <- 1000
t <- 10
lambda <- 3
alpha <- 2
beta <- 0.8
X10 <- numeric(total)
for(time in 1:total){
  set.seed(1234*time)
  N <- rpois(1,lambda*t)
  Y <- rgamma(N,shape = alpha,scale = beta)
  X10[time] <- sum(Y)
}
print(paste0("when lambda=",lambda,",alpha=",alpha,",beta=",beta))
print(paste0("the theoretical values:E[X(10)]=",lambda*t*alpha*beta,",Var(x(10))=",lambda*t*(alpha^2*beta^2+alpha*beta^2),";"))
print(paste0("the empirical values:E[X(10)]=",mean(X10),",Var(x(10))=",var(X10),";"))
```


```{r,eval=FALSE}
total <- 1000
t <- 10
lambda <- 3
alpha <- 5
beta <- 2
X10 <- numeric(total)
for(time in 1:total){
  set.seed(1234*time)
  N <- rpois(1,lambda*t)
  Y <- rgamma(N,shape = alpha,scale = beta)
  X10[time] <- sum(Y)
}
print(paste0("when lambda=",lambda,",alpha=",alpha,",beta=",beta))
print(paste0("the theoretical values:E[X(10)]=",lambda*t*alpha*beta,",Var(x(10))=",lambda*t*(alpha^2*beta^2+alpha*beta^2),";"))
print(paste0("the empirical values:E[X(10)]=",mean(X10),",Var(x(10))=",var(X10),";"))
```

We can know that the empirical values of the simulation are close to the theoretical values.




## Homework-2024.09.23


### Question

1. Write a function to compute a Monte Carlo estimate of the $Beta(3,3)$ cdf, and use the function to estimate $F(x)$ for $x=0.1,0.2,\dots,0.9$. Compare the estimates with the values returned by the pbeta function in R.


2. The Rayleigh density [156,(18.76)] is
$$
f(x) = \frac{x}{\sigma^2} e^{−x^2/(2\sigma^2)} \qquad x ≥ 0,σ> 0.
$$

Implement a function to generate samples from a Rayleigh($\sigma$) distribution, using antithetic variables. What is the percent reduction in variance of $\frac{X+X^{'}}{2}$ compared with $\frac{X_1+X_2}{2}$ for independent $X_1,X_2$? 


3. Find two importance functions $f_1$ and $f_2$ that are supported on $(1,\infty)$ and are 'close' to
$$
g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},\qquad x>1.
$$

Which of your two importance functions should produce the smaller variance in estimating
$$
\int_1^{\infty} \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2} dx
$$
by importance sampling? Explain. 


4. Monte Carlo experiment

(a). For $n=10^4,2\times 10^4,4\times 10^4,6\times 10^4,8\times 10^4$,  apply the fast
sorting algorithm to randomly permuted numbers of $1,\dots,n$.

(b). Calculate computation time averaged over $100$ simulations, denoted by $a_n$.

(c). Regress $a_n$ on $t_n := n\log (n) $, and graphically show the results(scatter plot and regression line). 


### Answer

1. The probability density function of the $Beta(\alpha,\beta)$ distribution is given by
$$
f(x)=\frac{1}{B(\alpha,\beta)} x^{\alpha-1}(1-x)^{\beta-1},\qquad \alpha>0,\beta>0.
$$

The cumulative distribution function can be expressed as
$$
\begin{aligned}
F(x) &= \int_0^x f(u)du \\
&= \int_0^x \frac{1}{B(\alpha,\beta)} u^{\alpha-1}(1-u)^{\beta-1}.
\end{aligned}
$$

The parameter we want to estimate is
$$
\theta=\int_0^x f(u)du=\int_0^x \frac{1}{x} \cdot xf(u)du = \mathbb{E} [g(U)],
$$
where $U \sim U(0,x)$,$g(u)=xf(u)$.

```{r,eval=FALSE}
N <- 1000
alpha <- 3
beta <- 3
Fhat <- function(x){
  set.seed(1234)
  U <- runif(N,0,x)
  gU <- x * 1/beta(alpha,beta)*U^(alpha-1)*(1-U)^(beta-1)
  return(mean(gU))
}
```

```{r,eval=FALSE}
for(x in (1:9)/10){
  print(paste0("when x = ",x,",the real F(x):",pbeta(x,alpha,beta),",the estimated F(x):",Fhat(x),"."))
}
```

From the Monte Carlo results, we can see that our estimates are close to the true values.


2. To generate samples from the Rayleigh distribution using the inverse transform method, we first need to calculate its corresponding cumulative distribution function (CDF).

$$
\begin{aligned}
F(x) &= \int_0^x f(x)dx \\
&=\int_0^x \frac{x}{\sigma^2} e^{−x^2/(2\sigma^2)} dx \\
&= 1-e^{−x^2/(2\sigma^2)}
\end{aligned}
$$

then, 
$$
\begin{aligned}
u&=F(x)\\
u&=1-e^{−x^2/(2\sigma^2)} \\
e^{−x^2/(2\sigma^2)}&=1-u \\
x^2 &= -2\sigma^2 \cdot \log (1-u) \\
x&= \sqrt{ -2\sigma^2 \cdot \log (1-u) }
\end{aligned}
$$

Then, according to the inverse transform method:

a. Generate a variable $U \sim U(0,1)$,$U^{'}=1-U$;

b. Obtain $X=F_X^{-1}(U)$,$X^{'}=F_X^{-1}(U^{'})$.

```{r,eval=FALSE}
invF <- function(u,sigma){
  x <- sqrt(  -2*sigma^2 * log(1-u) )
  return(x)
}
N <- 1000
set.seed(1234)
U <- runif(N,0,1)
X1 <- rep(0,N/2)
X2 <- rep(0,N/2)
X <- rep(0,N)
```

```{r,eval=FALSE}
sigma <- 1
X1 <- invF(U[1:(N/2)],sigma)
X2 <- invF(U[(N/2+1):N],sigma)
AX <- invF(1-U[1:(N/2)],sigma)
svar1 <- sd((X1+X2)/2)^2 * N / (N-1)
svar2 <- sd((X1+AX)/2)^2 * N /(N-1)
print(paste0("The variance reducts ",(1-svar2/svar1)*100,"%."))
```

3. $g(x)$ is defined on $(1,\infty)$. We choose two density functions as our importance functions.

First, we select the half-normal distribution on $(1,\infty)$: 
$$
f_1(x)=\sqrt{\frac{2}{\pi}}e^{-\frac{(x-1)^2}{2}},x>1.
$$

Then, we chose the exponential distribution on $(1,\infty)$: 
$$
f_2(x)=\lambda e^{-\lambda(x-1)},x>1.
$$

```{r,eval=FALSE}
g <- function(x){
  result <- x^2/sqrt(2*pi) * exp(-x^2/2)
  return(result)
}
f1 <- function(x){
  result <- sqrt(2/pi) * exp(-(x-1)^2/2)
  return(result)
}
f2 <- function(x,lambda){
  result <- lambda * exp(-lambda*(x-1))
}
```


```{r,eval=FALSE}
N <- 10000
set.seed(1234)
X <- abs(rnorm(N,0,1)) + 1
mean(g(X)/f1(X))

lambda <- 1 ##parameter of the exponential distribution
set.seed(1234)
Y <- rexp(N,lambda)+1
mean(g(Y)/f2(Y,lambda))
```

importance function|$f_1(x)$|$f_2(x)$
:-:|:-:|:-:
$\int_1^{\infty} g(x)dx$|0.4003865|0.4006047


4. 

The quick sort function is as follows: 
```{r,eval=FALSE}
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))}#??????
}
```

We set $n=10^4,2\times 10^4,4\times 10^4,6\times 10^4,8\times 10^4$, and calculate computation time. 
```{r,eval=FALSE}
all <- 100
time1 <- numeric(all)
time2 <- numeric(all)
time4 <- numeric(all)
time6 <- numeric(all)
time8 <- numeric(all)
for(i in 1:all){
  test1 <- sample(1:1e4)
  test2 <- sample(1:(2*1e4))
  test4 <- sample(1:(4*1e4))
  test6 <- sample(1:(6*1e4))
  test8 <- sample(1:(8*1e4))
  time1[i] <- system.time(quick_sort(test1))[1]
  time2[i] <- system.time(quick_sort(test2))[1]
  time4[i] <- system.time(quick_sort(test4))[1]
  time6[i] <- system.time(quick_sort(test6))[1]
  time8[i] <- system.time(quick_sort(test8))[1]
}
time <- c(mean(time1),mean(time2),mean(time4),mean(time6),mean(time8))
time
```

n|$1e4$|$2 \times 1e4$|$4 \times 1e4$|$6 \times 1e4$|$8 \times 1e4$
:-:|:-:|:-:|:-:|:-:|:-:
time|0.0133|0.0236|0.0639|0.0848|0.1195


The regression results between $a_n$ and $t_n := n \log(n)$ are as follows: 
```{r,eval=FALSE}
n <- c(1,2,4,6,8)*1e4
true <- n*log(n)
lm(time~true)
plot(true,time)
abline(coef(lm(time~true))[1],coef(lm(time~true))[2],col="red")
```


## Homework-2024.09.30


### Question

1. Estimate the $0.025,0.05,0.95,$ and $0.975$ quantiles of the skewness $\sqrt{b_1}$ under normality by a Monte Carlo experiment. Compute the standard error of the estimates from $(2.14)$ using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation $\sqrt{b_1} \approx \mathcal{N}(0,6/n)$. 


2. Tests for association based on Pearson product moment correlation $\rho$, Spearman's rank correlation coefficient $\rho_s$, or Kendall's coefficient $\tau$, are implemented in $\textbf{cor.test}$. Show (empirically) that the nonparametric tests based on $\rho_s$ or $\tau$ are less powerful than the correlation test when the sampled distribution is bivariate normal. Find an example of an alternative(a bivariate distribution (X,Y) such that X and Y are dependent) such that at least one of the nonparametric tests have bette emoirical power than the correlation test against this alternative.


3. If we obtain the powers for two methods under a particular simulation setting with $10,000$ experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at $0.05$ level.

(a). What is the corresponding hypothesis test problem? 

(b). What test should we use? Z-test,two-sample t-test,paired-t test or McNemar test? Why? 

(c). Please provide the least necessary information for hypothesis testing.


### Answer


1. We first set the sample size $n = 1000$ and repeat the experiment $N$ times to obtain $N$ estimates of skewness. For the skewness, we use the method of moments for estimation. 
$$
\sqrt{b_1}=\frac{\mu_3}{\mu_2^{3/2}},\qquad \mu_k=\mathbb{E}[X-\mathbb{E}(X)]^k.
$$



```{r,eval=FALSE}
rm(list=ls())
n <- 1000
N <- 1000

skew <- numeric(N)
for(i in 1:N){
  set.seed(i)
  X <- rnorm(n,0,1)
  m2 <- mean((X-mean(X))^2)
  m3 <- mean((X-mean(X))^3)
  skew[i] <- m3/(m2^(3/2))
}
hist(skew,probability = TRUE,xlim=c(-0.3,0.3),ylim = c(0,5),breaks = 12)
y <- seq(-0.3,0.3,0.01)
lines(y,1/(sqrt(2*pi*6/n))*exp(-y^2/(2*6/n)),col="red")
```

```{r,eval=FALSE}
q <- c(0.025,0.05,0.95,0.975)
quantile(skew,probs=q)
qnorm(q,0,sqrt(6/n))
```

According to the equation$(2.14)$, the variance of the $q$ sample quantile is
$$
Var(\hat{x}_q)=\frac{q(1-q)}{nf(x_q)^2},
$$
where $f$ is the density of the sampled distribution.

Since the distribution $f$ is unknown, we use the normal distribution to approximate the estimate of $f(x_q)$. Therefore, we first need to estimate the sample mean and variance of the skewness data. 
```{r,eval=FALSE}
smean <- mean(skew)
svar <- 1/(N-1)*sum((skew-mean(skew))^2)
fq <- dnorm(quantile(skew,probs=c(0.025,0.05,0.95,0.975)),mean = smean,sd=sqrt(svar))
varq <- q*(1-q)/(N*fq^2)
varq
```

quantile|0.025|0.05|0.95|0.975
:-:|:-:|:-:|:-:|:-:
Skewness quantile|-0.1515948|-0.1272963|0.1286182|0.1516004
Approximate normal quantile|-0.1518182|-0.1274098|0.1274098|0.1518182
Estimate variance|4.010600e-05|2.567798e-05|2.817547e-05|4.191878e-05 



2. We first generate a random variable following a bivariate normal distribution, considering the case where the alternative hypothesis is true.

Clear memory before calling the function.
```{r,eval=FALSE}
rm(list = ls())
```


```{r,eval=FALSE}
n <- 1000
mean <- c(0,0)
alpha <- 0.05
cor <- 0.1
sigma1 <- 1
sigma2 <- 2
sigma <- matrix(c(sigma1^2,cor*sigma1*sigma2,cor*sigma1*sigma2,sigma2^2),nrow = 2)
N <- 1000
reject1 <- numeric(N)
reject2 <- numeric(N)
reject3 <- numeric(N)
for(i in 1:N){
  set.seed(i)
  data <- rmvnorm(n,mean = mean,sigma = sigma) ##data generation
  p1 <- cor.test(data[,1],data[,2],alternative = "two.sided",method = c("pearson"))$p.value
  p2 <- cor.test(data[,1],data[,2],alternative = "two.sided",method = c("spearman"))$p.value
  p3 <- cor.test(data[,1],data[,2],alternative = "two.sided",method = c("kendall"))$p.value
  reject1[i] <- as.numeric(p1<alpha) ##rejection
  reject2[i] <- as.numeric(p2<alpha)
  reject3[i] <- as.numeric(p3<alpha)
}
```

The power of the three tests is as follows:
```{r,eval=FALSE}
sum(reject1)/N ##power
sum(reject2)/N
sum(reject3)/N
```

test|Pearson|Spearman|Kendall
:-:|:-:|:-:|:-:
power|0.873|0.849|0.85

It can be seen that the power of the Pearson test is higher than that of the Spearman test and the Kendall test when the samples follow the bivariate normal distribution.


Consider the bivariate exponential distribution, implemented through a custom function, while also considering the case where the alternative hypothesis is true.

Clear memory before calling the function.
```{r,eval=FALSE}
rm(list = ls())
```

Custom function to generate bivariate exponential distribution。
```{r,eval=FALSE}
generate_bivariate_exponential <- function(n, lambda1, lambda2, correlation) {
  u1 <- rexp(n, rate = lambda1)  # first variable
  u2 <- rexp(n, rate = lambda2)  # second variable
  
  # adjust the second variable through correlation
  v <- correlation * u1 + sqrt(1 - correlation^2) * u2
  
  return(data.frame(X = u1, Y = v))
}
```


```{r,eval=FALSE}
n <- 1000
alpha <- 0.05
lambda1 <- 1
lambda2 <- 2
correlation <- 0.05
N <- 1000
reject1 <- numeric(N)
reject2 <- numeric(N)
reject3 <- numeric(N)
for(i in 1:N){
  set.seed(i)
  data <- generate_bivariate_exponential(n, lambda1, lambda2, correlation) #data generation
  p1 <- cor.test(data$X,data$Y,alternative = "two.sided",method = c("pearson"))$p.value
  p2 <- cor.test(data$X,data$Y,alternative = "two.sided",method = c("spearman"))$p.value
  p3 <- cor.test(data$X,data$Y,alternative = "two.sided",method = c("kendall"))$p.value
  reject1[i] <- as.numeric(p1<alpha) #rejection
  reject2[i] <- as.numeric(p2<alpha)
  reject3[i] <- as.numeric(p3<alpha)
}
```

The power of the three tests is as follows:
```{r,eval=FALSE}
sum(reject1)/N ##power
sum(reject2)/N
sum(reject3)/N
```

test|Pearson|Spearman|Kendall
:-:|:-:|:-:|:-:
power|0.912|0.988|0.989

It can be seen that the power of the Pearson test is lower than that of the Spearman test and the Kendall test when the samples follow the bivariate exponential distribution.


3. 

(a). Based on the problem statement, we aim to test whether the power of the two methods differs at the significance level of \(0.05\). Therefore, the corresponding hypothesis testing problem is:

$$
H_0: \text{There is no difference in the power of the two test methods.}
$$
$$
H_1: \text{There is a difference in the power of the two test methods.}
$$

Let \( p_1 \) represent the power of Method 1 and \( p_2 \) represent the power of Method 2. Then, the hypothesis testing problem is:
$$
H_0:p_1=p_2. \qquad H_1:p_1\neq p_2.
$$

(b). 

$\textbf{The z-test}$ is generally used to compare the differences in sample means for large samples, and it uses the theory of the standard normal distribution to infer the probability of such differences occurring.

$\textbf{The two-sample t-test}$ is commonly used to compare whether the means of two independent samples are significantly different.

$\textbf{The paired t-test}$ compares the means of two related samples to determine whether there is a significant difference between the two groups.

$\textbf{The McNemar test}$ is a paired test for \(2 \times 2\) contingency tables, using each subject as their own control to test whether there are significant changes between two groups.

Since we aim to compare the power of two test methods across \(10,000\) experiments based on the samples from each experiment, it is clear that the power of the tests does not asymptotically follow a normal distribution in large samples. Moreover, the results of the two test methods from each experiment are not independent. Additionally, we are not dealing with a \(2 \times 2\) paired data table. Therefore, using $\textbf{the paired t-test}$ is the most appropriate choice.


(c). For the hypothesis testing problem, the necessary information we need to know includes:

\begin{itemize}
\item Significance level \( \alpha \)

\item Sample size \( n \)

\item Difference in means of the two samples

\item Standard deviation of the sample differences
\end{itemize}

According to the problem, the significance level is \( \alpha = 0.05 \), the sample size is \( n = 10,000 \), and the powers under the two tests are \( p_1 = 0.651 \) and \( p_2 = 0.676 \).

Let the results of each experimental sample under the two test methods be \( A_i, B_i \in \{0, 1\} \). Then, we have \( p_1 = \frac{1}{n} \sum_{i=1}^n A_i \) and \( p_2 = \frac{1}{n} \sum_{i=1}^n B_i \). The variance of the sample differences is given by \( s^2 = \frac{1}{n-1} \sum_{i=1}^n (A_i - B_i - (0.651 - 0.676))^2 \). The maximum variance is \( s^2_{max} = 1.05073 \) and the minimum variance is \( s^2_{min} = 0.00300 \). Therefore, the minimum value of the test statistic \( t \) is calculated as follows:
\[
t = \frac{0.025}{\sqrt{1.05073/10000}} \approx 2.44 > t_{\alpha/2} = 1.96.
\]

Thus, we reject the null hypothesis, concluding that there is a significant difference in the powers of the two tests. 


## Homework-2024.10.14


### Question

1. Of $N=1000$ hypotheses, $950$ are null and $50$ are alternative. The p-value under any null hypothesis is uniformly distributed(use runif), and the p-value under any alternative hypothesis follows the beta distribution with parameter $0.1$ and $1$ (use rbeta). Obtain Bonferroni adjusted p-values and B-H adjusted
p-values. Calculate FWER, FDR, and TPR under nominal level $\alpha = 0.1$ for each of the two adjustment methods based on $m = 10000$ simulation replicates. You should output the $6$ numbers (3) to a $3 \times 2$ table (column names: Bonferroni correction, B-H correction; row names: FWER, FDR, TPR). Comment the results. 


2.  Refer to the air-conditioning data set aircondit provided in the boot package. The $12$ observations are the times in hours between failures of airconditioning equipment [63, Example 1.1]:
$$
3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.
$$
Assume that the times between failures follow an exponential model Exp($\lambda$). Obtain the MLE of the hazard rate λ and use bootstrap to estimate the bias and standard error of the estimate.


3.  Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ. 


4. Suppose the population has the exponential distribution with rate $\lambda$, then the MLE of $\lambda$ is $\hat{\lambda}=1/ \bar{X}$, where $\bar{X}$ is the sample mean. It can be derived that the expectation of $\hat{\lambda}$ is $\lambda n /(n-1)$, so that the estimation bias is $\lambda /(n − 1)$. The standard error $\hat{\lambda}$ is $\lambda n / [(n-1)\sqrt{n-2}]$. Conduct a simulation study to verify the performance of the bootstrap method.

\begin{itemize}
\item The true value of $\lambda = 2$.

\item The sample size $n = 5, 10, 20$.

\item The number of bootstrap replicates $B = 1000$.

\item The simulations are repeated for $m = 1000$ times.

\item Compare the mean bootstrap bias and bootstrap standard error with the theoretical ones. Comment on the results.
\end{itemize}



### Solution


1.

$$
\text{FWER} = P(\text{at least one } H_0 \text{ is incorrectly rejected})
$$

$$
\text{FDR} = \frac{\text{Number of False Positives}}{\text{Total Number of Rejected Hypotheses}}
$$

$$
\text{TPR} = \frac{\text{Number of True Positives}}{\text{Total Number of Alternative Hypotheses}}
$$

```{r,eval=FALSE}
N <- 1000
n_null <- 950
n_alt <- N - n_null
alpha <- 0.1
m <- 10000

results <- matrix(0, nrow = 3, ncol = 2)
colnames(results) <- c("Bonferroni correction", "B-H correction")
rownames(results) <- c("FWER", "FDR", "TPR")

for (i in 1:m) {
  
  set.seed(i)
  p_null <- runif(n_null,0,1)
  p_alt <- rbeta(n_alt, 0.1, 1)
  p_values <- c(p_null, p_alt)
  
  bonferroni_p <- p.adjust(p_values, method = "bonferroni")
  bh_p <- p.adjust(p_values, method = "BH")
  
  bonferroni_rejects <- which(bonferroni_p < alpha)
  fwer_bon <- sum(bonferroni_rejects %in% 1:n_null) > 0
  fdr_bon <- sum(bonferroni_rejects %in% 1:n_null) / max(length(bonferroni_rejects), 1)
  tpr_bon <- sum(bonferroni_rejects %in% (n_null + 1):N) / n_alt
  
  bh_rejects <- which(bh_p < alpha)
  fwer_bh <- sum(bh_rejects %in% 1:n_null) > 0  # If at least one null hypothesis is incorrectly rejected, it is recorded as a false positive event.
  fdr_bh <- sum(bh_rejects %in% 1:n_null) / max(length(bh_rejects), 1) # Calculate how many of the rejected hypotheses are null hypotheses.
  tpr_bh <- sum(bh_rejects %in% (n_null + 1):N) / n_alt # Calculate the number of correctly rejected alternative hypotheses divided by the total number of true alternative hypotheses.
  
  results[1, 1] <- results[1, 1] + fwer_bon
  results[2, 1] <- results[2, 1] + fdr_bon
  results[3, 1] <- results[3, 1] + tpr_bon
  
  results[1, 2] <- results[1, 2] + fwer_bh
  results[2, 2] <- results[2, 2] + fdr_bh
  results[3, 2] <- results[3, 2] + tpr_bh
}

results <- results / m
print(round(results, 4))
```
It can be seen that the Bonferroni correction is very effective in controlling Type I errors, while the B-H correction performs poorly. For FDR, the Bonferroni correction is more conservative, which may lead to many true alternative hypotheses not being recognized, while the FDR value of the B-H correction is relatively high. Regarding TPR, the value obtained from the Bonferroni correction is low, indicating insufficient capability in identifying true alternative hypotheses, whereas the B-H correction shows stronger ability in recognizing true alternative hypotheses.


2. 

From Exercise 4, we know that the MLE of \(\lambda\) is \(\hat{\lambda} = 1/\bar{X}\).
```{r,eval=FALSE}
aircondit_times <- c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
sample_mean <- mean(aircondit_times)
lambda_mle <- 1 / sample_mean
lambda_mle_boot <- function(data, indices) {
  sample_data <- data[indices]
  return(1 / mean(sample_data))
}
boot_results <- boot(data = aircondit_times, statistic = lambda_mle_boot, R = 1000)
print(boot_results)
lambda_bias <- mean(boot_results$t) - lambda_mle
lambda_se <- sd(boot_results$t)

paste0("MLE of lambda:", lambda_mle, "\n")
paste0("Bias estimate:", lambda_bias, "\n")
paste0("Standard error estimate:", lambda_se, "\n")

```


3. 

```{r,eval=FALSE}
mu_boot <- function(data, indices) {
  sample_data <- data[indices]
  return(mean(sample_data))  
}
boot_results <- boot(data = aircondit_times, statistic = mu_boot, R = 1000)

normal_ci <- boot.ci(boot_results, type = "norm")
basic_ci <- boot.ci(boot_results, type = "basic")
percentile_ci <- boot.ci(boot_results, type = "perc")
bca_ci <- boot.ci(boot_results, type = "bca")

paste0("Normal 95% CI:", normal_ci$normal[2:3], "\n")
paste0("Basic 95% CI:", basic_ci$basic[4:5], "\n")
paste0("Percentile 95% CI:", percentile_ci$percent[4:5], "\n")
paste0("BCa 95% CI:", bca_ci$bca[4:5], "\n")

```

The normal confidence interval assumes that the distribution of the sample mean is normal and calculates the confidence interval based on the standard error of the sample mean, making it suitable for larger sample sizes.

The basic confidence interval is directly calculated based on the mean and standard error of the bootstrap samples, making it applicable when the distribution of the original data is uncertain.

The percentile confidence interval directly determines the confidence interval by taking the percentiles of the bootstrap samples.

The BCa confidence interval applies bias correction and acceleration to the basic confidence interval, enhancing its accuracy.

In this case, the widths of the four confidence intervals do not differ significantly.

4.

```{r,eval=FALSE}
lambda_true <- 2
sizes <- c(5, 10, 20)
B <- 1000  
m <- 1000 

results <- data.frame(sample_size = integer(),
                      mean_bootstrap_bias = numeric(),
                      bootstrap_se = numeric(),
                      theoretical_bias = numeric(),
                      theoretical_se = numeric())

for (n in sizes) {
  mean_bias <- numeric(m)
  bootstrap_se <- numeric(m)
  
  for (i in 1:m) {
    set.seed(i)
    sample_data <- rexp(n, rate = lambda_true)
    boot_results <- boot(data = sample_data, statistic = lambda_mle_boot, R = B)
    mean_bias[i] <- mean(boot_results$t) - (1 / mean(sample_data))
    bootstrap_se[i] <- sd(boot_results$t)
  }
  
  mean_bootstrap_bias_avg <- mean(mean_bias)
  bootstrap_se_avg <- mean(bootstrap_se)
  
  theoretical_bias <- lambda_true / (n - 1)
  theoretical_se <- (lambda_true * n) / ((n - 1) * sqrt(n - 2))
  
  results <- rbind(results,
                    data.frame(sample_size = n,
                               mean_bootstrap_bias = mean_bootstrap_bias_avg,
                               bootstrap_se = bootstrap_se_avg,
                               theoretical_bias = theoretical_bias,
                               theoretical_se = theoretical_se))
}
print(results)

```

It can be seen that as the sample size increases, the bootstrap bias and standard deviation become closer to the theoretical values.


## Homework-2024.10.21



### Question

1. Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$, 


2. In Example 7.18, leave-one-out($n$-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^2$?


3. Implement the two-sample Cramer-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.


4. Implement the bivariate Spearman rank correlation test for independence[255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the $p$-value reported by cor.test on the same samples.



### Solution

1.

The Jackknife method is used to estimate the bias and standard error of a statistic. Each observation can be removed once to calculate the \(\hat{\lambda}\) value for each sample, which allows the calculation of \(\hat{\theta}\) values, and then the overall bias and standard error can be computed. 
```{r,eval=FALSE}
rm(list=ls())
scor


cov_scor <- cov(scor)
lambda <- eigen(cov_scor)$values
theta_hat <- lambda [1] / sum(lambda )
n <- nrow(scor)
theta_jackknife <- numeric(n)

for (i in 1:n) {
  cov_jack <- cov(scor[-i, ])
  lambda_jack <- eigen(cov_jack)$values
  theta_jackknife[i] <- lambda_jack[1] / sum(lambda_jack)
}

bias_jack <- (n - 1) * (mean(theta_jackknife) - theta_hat)
se_jack <- sqrt((n - 1) * mean((theta_jackknife - theta_hat)^2))

round(c(original=theta_hat,bias.jack=bias_jack,
se.jack=se_jack),5)


```



2.

Cubic:
$$
Y=\beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon.
$$

```{r,eval=FALSE}
rm(list=ls())
attach(ironslag)
a <- seq(10, 40, .1)
L1 <- lm(magnetic ~ chemical)
plot(chemical, magnetic, main="Linear", pch=16)
yhat1 <- L1$coef[1] + L1$coef[2] * a
lines(a, yhat1, lwd=2)
L2 <- lm(magnetic ~ chemical + I(chemical^2))
plot(chemical, magnetic, main="Quadratic", pch=16)
yhat2 <- L2$coef[1] + L2$coef[2] * a + L2$coef[3] * a^2
lines(a, yhat2, lwd=2)
L3 <- lm(log(magnetic) ~ chemical)
plot(chemical, magnetic, main="Exponential", pch=16)
logyhat3 <- L3$coef[1] + L3$coef[2] * a
yhat3 <- exp(logyhat3)
lines(a, yhat3, lwd=2)
L4 <- lm(magnetic ~ chemical + I(chemical^2) + I(chemical^3))
plot(chemical, magnetic, main="Cubic", pch=16)
yhat4 <- L4$coef[1] + L4$coef[2] * a + L4$coef[3] * a^2 + L4$coef[4] * a^3
lines(a, yhat4, lwd=2)
```

```{r,eval=FALSE}
n <- length(magnetic)
e1 <- e2 <- e3 <- e4 <- numeric(n)
for(k in 1:n) {
  y <- magnetic[-k]
  x <- chemical[-k]
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
  e1[k] <- magnetic[k] - yhat1
  J2 <- lm(y ~ x + I(x^2))
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
  J2$coef[3] * chemical[k]^2
  e2[k] <- magnetic[k] - yhat2
  J3 <- lm(log(y) ~ x)
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
  yhat3 <- exp(logyhat3)
  e3[k] <- magnetic[k] - yhat3
  J4 <- lm(y ~ x + I(x^2) + I(x^3))
  yhat4 <- J4$coef[1] + J4$coef[2] * chemical[k] + J4$coef[3] * chemical[k]^2 + J4$coef[4] * chemical[k]^3
  e4[k] <- magnetic[k] - yhat4
}
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
```

By comparing the prediction errors of each model, we can see that the cross-validation procedure still selects model 2.

```{r,eval=FALSE}
L2
```

\[
\text{adjusted } R^2 = 1 - (1 - R^2) \frac{n - 1}{n - p - 1}
\]

 Here, \(n\) is the sample size, and \(p\) is the number of parameters in the model. 



```{r,eval=FALSE}
p1 <- 1
R2_1 <- summary(J1)$r.squared
adj_R2_1 <- 1 - (1 - R2_1) * (n - 1) / (n - p1 - 1)

p2 <- 2
R2_2 <- summary(J2)$r.squared
adj_R2_2 <- 1 - (1 - R2_2) * (n - 1) / (n - p2 - 1)

p3 <- 1
R2_3 <- summary(J3)$r.squared
adj_R2_3 <- 1 - (1 - R2_3) * (n - 1) / (n - p3 - 1)

p4 <- 3
R2_4 <- summary(J4)$r.squared
adj_R2_4 <- 1 - (1 - R2_4) * (n - 1) / (n - p4 - 1)

c(adj_R2_1,adj_R2_2,adj_R2_3,adj_R2_4)
```

According to the maximum adjusted \(R^2\), model 4 is selected. 

```{r,eval=FALSE}
L4
```



3. 

The Cramer-von Mises test is used to compare whether two distributions are the same. This is implemented through a permutation test, which randomly samples from the combined samples to obtain multiple sets of data and computes the Cramer-von Mises statistic for each group, comparing it to the observed statistic to estimate the significance level. 


```{r,eval=FALSE}
rm(list=ls())
attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
detach(chickwts)
cvm_statistic <- function(x, y) {
  n <- length(x)
  m <- length(y)
  combined <- c(sort(x), sort(y))
  ecdf_x <- cumsum(combined %in% x) / n
  ecdf_y <- cumsum(combined %in% y) / m
  D <- sum((ecdf_x - ecdf_y)^2) * (n * m) / (n + m)^2  # 归一化
  return(D)
}
D0 <- cvm_statistic(x,y)
R <- 999
z <- c(x,y)
n1 <- length(x)
n2 <- length(y)
D <- numeric(R)
for (i in 1:R) {
  set.seed(i)
  permuted_indices <- sample(length(z))
  x1 <- z[permuted_indices[1:n1]]
  x2 <- z[permuted_indices[(n1 + 1):(n1 + n2)]]
  D[i] <- cvm_statistic(x1, x2)
}

p_value <- mean(c(D0, D) >= D0)
p_value

```


4.


We generate two samples \((X,Y)\) that follow \(H_0\) and \(H_1\):
\[
X_{H_0} \sim \mathcal{N}(0,1) \quad Y_{H_0} \sim \mathcal{N}(0,1),
\]
\[
X_{H_1} \sim \mathcal{N}(0,1) \quad Y_{H_1} \sim X_{H_1} + \mathcal{N}(0,0.5^2).
\]

The function `spearman_permutation_test` defined here computes the Spearman rank correlation coefficient and the p-value of the permutation test.

```{r,eval=FALSE}
rm(list=ls())
set.seed(1234)
n <- 100
X_h0 <- rnorm(n)
Y_h0 <- rnorm(n)
X_h1 <- rnorm(n)
Y_h1 <- X_h1 + rnorm(n, mean = 0, sd = 0.5) 
spearman_permutation_test <- function(x, y, R = 999) {
  original_correlation <- cor(x, y, method = "spearman")
  perm_correlations <- numeric(R)
  for (i in 1:R) {
    set.seed(i)
    perm_y <- sample(y)
    perm_correlations[i] <- cor(x, perm_y, method = "spearman")
  }
  p_value <- mean(abs(perm_correlations) >= abs(original_correlation))
  return(list(correlation = original_correlation, p_value = p_value))
}

result_h0 <- spearman_permutation_test(X_h0, Y_h0)
result_h1 <- spearman_permutation_test(X_h1, Y_h1)
cor_test_h0 <- cor.test(X_h0, Y_h0, method = "spearman")
cor_test_h1 <- cor.test(X_h1, Y_h1, method = "spearman")

cat("Under H0:\n")
cat("Spearman correlation:", result_h0$correlation, "\n")
cat("permutation p-value:", result_h0$p_value, "\n")
cat("cor.test p-value:", cor_test_h0$p.value, "\n\n")

cat("Under H1:\n")
cat("Spearman correlation:", result_h1$correlation, "\n")
cat("permutation p-value:", result_h1$p_value, "\n")
cat("cor.test p-value:", cor_test_h1$p.value, "\n")

```


## Homework-2024.10.28


### Question


1. Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchy or qt with df=1). Recall that a Cauchy($\theta,\eta$) distribution has density function
$$
f(x)= \frac{ 1 }{ \theta \pi (1+[(x-\eta)/ \theta]^2) }, \quad -\infty < x < \infty,\theta>0.
$$

The standard Cauchy has the Cauchy($\theta=1,\eta=0$)density. (Note that the standard Cauchy density is equal to the Student $t$ density with one degree of freedom.)






2. This example appears in [40]. Consider the bivariate density
$$
f(x,y) \propto \binom{n}{x} y^{x+a-1} (1-y)^{n-x+b-1},\quad x=0,1,\dots,n, \, 0 \leq y \leq1.
$$
It can be shown (see e.g. [23]) that for fixed $a$,$b$,$n$, the conditional distributions are Binomial($n, y$) and Beta($x + a, n − x + b$). Use the Gibbs sampler to generate a chain with target joint density $f(x, y)$.


3. For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R} < 1.2$.


4. Algorithm(continuous situation)
\begin{itemize}
   \item Target pdf:$f(x)$,
   \item Replace $i$ and $j$ with $s$ and $r$,
   \item Proposal distribution (pdf):$g(r|s)$.
   \item Acceptance probability:$\alpha(s,r)=min\{ \frac{f(r)g(s|r)}{f(s)g(r|s)},1 \}$,
   \item Transition kernel(mixture distribution):
   $$
   K(r,s)=I(s \neq r) \alpha(r,s) g(s|r)+I(s=r)[1-\int \alpha(r,s)g(s|r)],
   $$
   \item Stationarity:$K(s,r)f(s)=K(r,s)f(r)$.
\end{itemize}


### Solution




1.

```{r,eval=FALSE}
rm(list=ls())
metropolis_hastings <- function(num_samples) {
  samples <- numeric(num_samples)
  current <- rnorm(1)
  for (i in 1:num_samples) {
    proposed <- rnorm(1, mean = current) 
    acceptance_ratio <- dcauchy(proposed) / dcauchy(current)
    if (runif(1) < acceptance_ratio) {
      current <- proposed
    }
    samples[i] <- current
  }
  return(samples)
}
num_samples <- 10000
set.seed(1234)
samples <- metropolis_hastings(num_samples)
samples <- samples[-(1:1000)]
deciles_generated <- quantile(samples, probs = seq(0.1, 1, by = 0.1))
deciles_cauchy <- qcauchy(seq(0.1, 1, by = 0.1))
cat("Deciles of generated samples:\n", deciles_generated, "\n")
cat("Deciles of standard Cauchy distribution:\n", deciles_cauchy, "\n")
samples_df <- data.frame(Value = samples)
ggplot(samples_df, aes(x = Value)) +
  geom_histogram(aes(y = after_stat(density)), bins = 50, fill = "lightblue", alpha = 0.7) +
  stat_function(fun = dcauchy, color = "red", size = 1) +
  labs(title = "Metropolis-Hastings Sampling from Standard Cauchy",
       x = "Value",
       y = "Density") +
  theme_minimal() +
  theme(legend.position = "topright") +
  annotate("text", x = 3, y = 0.1, label = "Standard Cauchy PDF", color = "red")
```

Here are the results of the deciles obtained from the generated samples compared to the standard Cauchy distribution.

\text{ }|Q1|Q2|Q3|Q4|Q5|Q6|Q7|Q8|Q9|Q10
:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:
generated samples|-2.634386|-1.074383|-0.494804|-0.1409595|0.2144038|0.5754213|1.056696|2.155424|6.717301|22.51901 
standard Cauchy|-3.077684|-1.376382|-0.7265425|-0.3249197|0|0.3249197|0.7265425|1.376382|3.077684|Inf


2.

```{r,eval=FALSE}
rm(list=ls())
gibbs_sampler <- function(n, a, b, num_samples) {
  samples <- data.frame(iteration = 1:num_samples, x = numeric(num_samples), y = numeric(num_samples))
  x <- 0 
  for (i in 1:num_samples) {
    y <- rbeta(1, x + a, n - x + b)
    x <- rbinom(1, n, y)
    samples[i, "x"] <- x
    samples[i, "y"] <- y
  }
  return(samples)
}
n <- 10  
a <- 1   
b <- 1   
num_samples <- 1000 
samples <- gibbs_sampler(n, a, b, num_samples)
samples_long <- reshape2::melt(samples, id.vars = "iteration")
ggplot(samples_long, aes(x = iteration, y = value, color = variable, group = variable)) +
  geom_line(size = 1) +
  labs(title = "Gibbs Sampler Chains for x and y",
       x = "Iteration",
       y = "Value") +
  scale_color_manual(values = c("x" = "lightblue", "y" = "lightgreen"), labels = c("x Chain", "y Chain")) +
  theme_minimal() +
  theme(legend.title = element_blank())
```

The plot displays the chains for $X$ and $Y$.


3.

```{r,eval=FALSE}
rm(list=ls())
```

\textbf{For problem1}

```{r,eval=FALSE}
metropolis_hastings <- function(num_samples) {
  samples <- numeric(num_samples)
  current <- rnorm(1)
  for (i in 1:num_samples) {
    proposed <- rnorm(1, mean = current) 
    acceptance_ratio <- dcauchy(proposed) / dcauchy(current)
    if (runif(1) < acceptance_ratio) {
      current <- proposed
    }
    samples[i] <- current
  }
  return(samples)
}
num_samples <- 10000
set.seed(1234)
chains <- lapply(1:3, function(i) metropolis_hastings(num_samples)[-c(1:1000)])
chains_mcmc <- mcmc.list(lapply(chains, mcmc))
gelman_diag_mh <- gelman.diag(chains_mcmc, autoburnin = FALSE)
print("Gelman-Rubin Diagnostic:")
print(gelman_diag_mh)
```


It can be seen that the obtained Gelman-Rubin statistic and the upper limit of the confidence interval are both less than 1.1, which is fine. 


\textbf{For problem2}

```{r,eval=FALSE}
gibbs_sampler <- function(n, a, b, num_samples) {
  samples <- data.frame(x = numeric(num_samples), y = numeric(num_samples))
  x <- 0 
  for (i in 1:num_samples) {
    y <- rbeta(1, x + a, n - x + b)
    x <- rbinom(1, n, y)
    samples[i, "x"] <- x
    samples[i, "y"] <- y
  }
  return(samples)
}
n <- 10
a <- 1
b <- 1
num_samples <- 1000
set.seed(1234)
chains_gibbs <- lapply(1:3, function(i) gibbs_sampler(n, a, b, num_samples))
chains_x <- lapply(chains_gibbs, function(chain) mcmc(chain$x))
chains_y <- lapply(chains_gibbs, function(chain) mcmc(chain$y))
chains_x_mcmc <- mcmc.list(chains_x)
chains_y_mcmc <- mcmc.list(chains_y)
gelman_diag_gibbs_x <- gelman.diag(chains_x_mcmc, autoburnin = FALSE)
gelman_diag_gibbs_y <- gelman.diag(chains_y_mcmc, autoburnin = FALSE)
print("Gelman-Rubin Diagnostic for X:")
print(gelman_diag_gibbs_x)
print("Gelman-Rubin Diagnostic for Y:")
print(gelman_diag_gibbs_y)
```

The Gelman-Rubin statistic and the upper limit of the confidence interval for both $X$ and $Y$ are also less than 1.1, which is fine.


4.

  
- When \( s \neq r \):
  \[
  K(r, s) = \alpha(r, s) g(s | r)
  \]

- When \( s = r \):
  \[
  K(r, s) = 1 - \int \alpha(r, s) g(s | r) \, ds
  \]

The acceptance probability is defined as:
\[
\alpha(s, r) = \min \left\{ \frac{f(r) g(s | r)}{f(s) g(r | s)}, 1 \right\}
\]

If \( f(r) g(s | r) \geq f(s) g(r | s) \), then \( \alpha(s, r) = 1 \); otherwise, if \( f(r) g(s | r) < f(s) g(r | s) \), then
\[
\alpha(s, r) = \frac{f(r) g(s | r)}{f(s) g(r | s)}
\]

To prove stationarity, i.e., that \( K(s, r) f(s) = K(r, s) f(r) \), we compute each side separately.

- **\( K(s, r) f(s) \)**:
  
  When \( s \neq r \),
  \[
  K(s, r) f(s) = \alpha(s, r) g(r | s) f(s)
  \]
  Substituting in \( \alpha(s, r) \), we obtain:
  \[
  K(s, r) f(s) = \min \left\{ f(r) g(s | r), f(s) g(r | s) \right\}
  \]

- **\( K(r, s) f(r) \)**:

  Similarly, for \( K(r, s) f(r) \), we obtain the same result:
  \[
  K(r, s) f(r) = \min \left\{ f(r) g(s | r), f(s) g(r | s) \right\}
  \]

This completes the proof.


## Homework-2024.11.04



### Question

1. 

(a).
Write a function to compute the $k$th term in
   $$
   \sum_{k=0}^{\infty} \frac{(-1)^k}{k! \cdot 2^k} \frac{\| a \|^{2k + 2}}{(2k + 1)(2k + 2)} \frac{\Gamma \left( \frac{d+1}{2} \right) \Gamma \left( k + \frac{3}{2} \right)}{\Gamma \left( k + \frac{d}{2} + 1 \right)},
   $$
   where $d \geq 1$ is an integer, $a$ is a vector in $\mathbb{R}^d$, and $\| \cdot \|$ denotes the Euclidean norm. Perform the arithmetic so that the coefficients can be computed for (almost) arbitrarily large $k$ and $d$. (This sum converges for all $a \in \mathbb{R}^d$.)


(b).
Modify the function so that it computes and returns the sum.

(c).
Evaluate the sum when $a = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$.



2.

Write a function to solve the equation
$$
\frac{2 \Gamma \left( \frac{k}{2} \right)}{\sqrt{\pi} (k - 1) \Gamma \left( \frac{k - 1}{2} \right)} \int_0^{c_{k-1}} \left( 1 + \frac{u^2}{k - 1} \right)^{-k/2} \, du = \frac{2 \Gamma \left( \frac{k+1}{2} \right)}{\sqrt{\pi k} \Gamma \left( \frac{k}{2} \right)} \int_0^{c_k} \left( 1 + \frac{u^2}{k} \right)^{-(k+1)/2} \, du
$$
for $a$, where
$$
c_k = \sqrt{\frac{a^2 k}{k + 1 - a^2}}.
$$
Compare the solutions with the points $A(k)$ in Exercise 11.4.


3.

Suppose $T_1,\dots,T_n$ are $i.i.d.$ samples drawn from the exponential distribution with expectation $\lambda$. Those values greater than $\tau$ are not observed due to right censorship, so that the observed values are $Y_i = T_i I(T_i \leq \tau) + \tau I(T_i > \tau), \, i= 1,\dots,n$. Suppose $\tau = 1$ and the observed $Y_i$ values are as follows:
$$
0.54, 0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85
$$
Use the E-M algorithm to estimate $\lambda$, compare your result with the observed data MLE (note: $Y_i$ follows a mixture distribution). 


### Solution

1.


(a).
```{r,eval=FALSE}
rm(list=ls())
compute_k <- function(k, a, d) {
  norm_a <- sqrt(sum(a^2)) 
  part1 <- ((-1)^k / (factorial(k) * 2^k)) * (norm_a^(2 * k + 2)) / ((2 * k + 1) * (2 * k + 2))
  part2 <- gamma((d + 1) / 2) * gamma(k + 3/2) / gamma(k + d / 2 + 1)
  result <- part1 * part2
  return(result)
}
```

We define the function \text{compute_k} to compute the $k$th term in the given series:



(b).
```{r,eval=FALSE}
compute_sum <- function(a, d, max = 150) {
  s <- 0
  for (k in 0:max) {
    s <- s + compute_k(k, a, d)
  }
  return(s)
}
```

The \text{compute_sum} function is used to calculate the infinite sum, approximated with a finite upper limit \text{max}. The default maximum number of terms, \text{max}=150, is set to avoid large values of $k$, which would cause the denominator of the $k$-th term to become too large, resulting in a \text{NaN} value during computation. 



(c).
```{r,eval=FALSE}
a <- c(1, 2)
d <- 2
result <- compute_sum(a, d)
print(result)
```

When $a=(1,2)^T$ and $d=2$, the function call \text{compute_sum(a,d)} yields the result 1.532164.


2.

First, we consider the intersection of two curves in Exercise 11.4 at $(0,\sqrt{k})$:
$$
S_{k-1}(a) = P \left( t(k - 1) > \sqrt{\frac{a^2 (k - 1)}{k - a^2}} \right)
$$

$$
S_k(a) = P \left( t(k) > \sqrt{\frac{a^2 k}{k + 1 - a^2}} \right).
$$

```{r,eval=FALSE}
rm(list=ls())
intersection <- function(k) {
  curve_1 <- function(a) 1 - pt( sqrt((a^2 * (k - 1)) / (k - a^2)) , df = k - 1 )
  curve_2 <- function(a) 1 - pt( sqrt((a^2 * k) / (k + 1 - a^2)) , df = k )
  intersection <- uniroot(function(x) curve_1(x) - curve_2(x), lower = 1, upper = 2)
  return(intersection$root)
}
k_values <- c(4:25, 100, 500, 1000)
points <- numeric(length(k_values))
for(i in 1:length(k_values)){
  points[i] <- intersection(k_values[i])
}
solve_equation <- function(k) {
  result <- tryCatch({
    uniroot(equation_diff, lower = 1, upper = sqrt(k) - 1e-6, k = k)
  }, error = function(e) {
    return(NULL)  
  })
  
  if (is.null(result)) {
    return(NA) 
  }
  
  return(result$root)
}
names(points) <- paste0("A(", k_values, ")")
print(points)
```

Here, since the root-finding interval is the open interval $(0,\sqrt{k})$，when using \text{uniroot}, the upper limit is set as \text{upper = sqrt(k)-1e-9} and the lower limit as \text{lower = 1e-9}.


Now, consider Problem 11.5:

```{r,eval=FALSE}
integrate_part <- function(c, k) {
  integral <- integral(function(u) (1 + (u^2) / k)^(-(k+1)/2), 0, c)
  return(integral)
}
c_k <- function(k,a){
  result <- sqrt( a^2 * k /( k + 1 - a^2 ) )
  return(result)
}
eq <- function(k,a){
  c <- c_k(k,a)
  result <- 2 * gamma((k+1)/2) / ( sqrt( pi * k ) * gamma( k / 2 ) ) * integrate_part(c,k)
  return(result)
}
intersection_1 <- function(k){
  lhs <- function(a) eq(k-1,a)
  rhs <- function(a) eq(k,a)
  intersection <- uniroot(function(x) lhs(x) - rhs(x), lower = 1+1e-1, upper = 1.8)
  return(intersection$root)
}
k_values_1 <- c(4:25,100) 
points_1 <- numeric(length(k_values_1))
for(i in 1:length(k_values_1)){
  points_1[i] <- intersection_1(k_values_1[i])
}
names(points_1) <- paste0("A(", k_values_1, ")")
print(points_1)
```

In 11.5, we set \text{k=c(4:25,100)}, since larger values of $k$ result in infinite values when calculating $\Gamma(\cdot)$. As we can observe, the solutions obtained are approximately the same as the results in 11.4.
```{r,eval=FALSE}
error <- points_1 - points[1:length(points_1)]
print(error)
```



3.

In this problem, we cannot fully observe the true value of each sample \( T_i \), so we need to use the EM algorithm to handle the unobserved part and estimate the parameter  \( \lambda \) .

First, we define the log-likelihood function:

Let

- \( \mathcal{O} = \{ i \mid Y_i < \tau \} \): the index set of uncensored samples.

- \( \mathcal{M} = \{ i \mid Y_i = \tau \} \): the index set of censored samples.

Then, the log-likelihood function is:
\[
\log L(\lambda) = \sum_{i \in \mathcal{O}} \log f(Y_i; \lambda) + \sum_{i \in \mathcal{M}} \log P(T_i > \tau; \lambda),
\]
where, \( f(Y_i; \lambda) = \lambda e^{- \lambda Y_i} \) is the probability density function of \( T_i \) under an exponential distribution, and \( P(T_i > \tau; \lambda) = e^{- \lambda \tau  } \).


We use the EM algorithm, which proceeds as follows:


For censored samples \( Y_i = \tau \)，, we need to compute the conditional expectation of \( T_i \) , \( \mathbb{E}[T_i \mid T_i > \tau, \lambda^{(k)}] \), given the current parameter estimate \( \lambda^{(k)} \):
$$
\begin{aligned}
\mathbb{E}[T_i \mid T_i > \tau, \lambda^{(k)}] &=  \int_{\tau}^{\infty} x \, f(x \mid X > \tau) \, dx \\
& =  \int_{\tau}^{\infty} x \, \frac{f(x)}{P(X > \tau)} I\{x > \tau\} \, dx \\
& = \int_{\tau}^{\infty} x \cdot \lambda^{(k)} e^{-\lambda^{(k)} (x - \tau)} \, dx \\
& = \int_{0}^{\infty} (u + \tau) \lambda^{(k)} e^{-\lambda^{(k)} u} \, du \\
& =\tau + \frac{1}{\lambda^{(k)}}
\end{aligned}
$$
  
For uncensored samples \( i \in \mathcal{O} \), we directly use\( Y_i \).
   
For censored samples \( i \in \mathcal{M} \), we replace \( T_i \) with its conditional expectation \( \tau + \frac{1}{\lambda^{(k)}} \).

Update the estimate of \( \lambda^{(k)} \) .
$$
\begin{aligned}
\lambda^{(k+1)} &= \frac{1}{ \frac{\sum_{i \in \mathcal{O}} Y_i + \sum_{i \in \mathcal{C}} \mathbb{E}[T_i \mid T_i > \tau, \lambda^{(k)}]}{n} } \\
& = \frac{1}{ \frac{\sum_{i \in \mathcal{O}} Y_i + \sum_{i \in \mathcal{C}} (\tau + \frac{1}{\lambda^{(k)}})}{n} }
\end{aligned}
$$


Set the initial value \( \lambda^{(0)} = 1 \), then iteratively compute until \( \lambda \) converges. 

```{r,eval=FALSE}
rm(list=ls())
T <- c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
tau <- 1
n <- length(T)
obs_indices <- which(T < tau)  
missing_indices <- which(T == tau) 
lambda <- 1 
error <- 1e-6  
max_iter <- 1000 
diff <- 1  # Set an initial difference
iter <- 0  # Set iteration counter
while (diff > error && iter < max_iter) {
  iter <- iter + 1
  lambda_old <- lambda
  ET_missing <- tau + 1 / lambda
  lambda <- n / (sum(T[obs_indices]) + length(missing_indices) * ET_missing)
  diff <- abs(lambda - lambda_old)
}
cat("EM algorithm estimation:", lambda, "\n")
lambda_mle <- 1 / mean(T[obs_indices])
cat("MLE estimation:", lambda_mle, "\n")

```

In this analysis, the difference between the EM algorithm estimate and the maximum likelihood estimate based on uncensored data is significant: The EM estimate yields \( \lambda_{\text{EM}} = 1.037037 \), while the maximum likelihood estimate based only on uncensored data is \( \lambda_{\text{MLE}} = 1.866667 \).



- Since some of the samples in the observed data are right-censored (i.e., \(T_i = 1\)), the EM algorithm fills in the values of these censored samples during the E-step using the conditional expectation, which results in more accurate estimates under censorship.
   
- On the other hand, the direct calculation of \(\lambda_{\text{MLE}}\) only considers uncensored samples and ignores the censored samples, leading to an overestimate.



## Homewrok-2024.11.11


### Question

1.

Use the simplex algorithm to solve the following problem.

Minimize $4x + 2y + 9z$ subject to
$$
2x+y+z \leq 2
$$
$$
x-y+3z \leq 3
$$
$$
x \geq 0,\, y \geq0, \, z\geq 0.
$$


2.

(1).

Use both for loops and \text{lapply()} to fit linear models to the \text{mtcars} using the formulas stored in this list:
```{r,eval=FALSE}
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
```


(2).

Fit the model \text{mpg ~ disp} to each of the bootstrap replicates of \text{mtcars} in the list below by using a for loop and \text{lapply()}. Can you do it without an anonymous function?
```{r,eval=FALSE}
bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
})
```


(3).

For each model in the previous two exercises, extract $R^2$ using the function below.
```{r,eval=FALSE}
rsq <- function(mod) summary(mod)$r.squared
```


3.

(1).

The following code simulates the performance of a t-test for non-normal data. Use \text{sapply()} and an anonymous function to extract the p-value from every trial.
```{r,eval=FALSE}
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
```

Extra challenge: get rid of the anonymous function by using [[ directly. 



(2).

Implement a combination of \text{Map()} and \text{vapply()} to create an \text{lapply()} variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?


4.

(1).

Make a faster version of \text{chisq.test()} that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying \text{chisq.test()} or by coding from the mathematical definition (http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)


(2).

Can you make a faster version of \text{table()} for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?



### Answer

1.
```{r,eval=FALSE}
rm(list=ls())
objective <- c(4, 2, 9)
constraints <- matrix(c(2, 1, 1, 1, -1, 3), nrow = 2, byrow = TRUE)
direction <- c("<=", "<=")
rhs <- c(2, 3)
result <- lp("min", objective, constraints, direction, rhs)
result$solution 
result$objval   

```


2.

(1).
```{r,eval=FALSE}
rm(list=ls())
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
for (f in formulas) {
  print(summary(lm(f, data = mtcars)))
}
lapply(formulas, function(f) summary(lm(f, data = mtcars)))
```


(2).
```{r,eval=FALSE}
bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
})
for (bootstrap in bootstraps) {
  print(summary(lm(mpg ~ disp, data = bootstrap)))
}
lapply(bootstraps, function(bootstrap) summary(lm(mpg ~ disp, data = bootstrap)))

```


(3).
```{r,eval=FALSE}
rsq <- function(mod) summary(mod)$r.squared
for (f in formulas) {
  mod <- lm(f, data = mtcars)
  print(rsq(mod))
}
print(lapply(formulas, function(f) rsq(lm(f, data = mtcars))))


for (bootstrap in bootstraps) {
  mod <- lm(mpg ~ disp, data = bootstrap)
  print(rsq(mod))
}
print(lapply(bootstraps, function(bootstrap) rsq(lm(mpg ~ disp, data = bootstrap))))
```


3.

(1).
```{r,eval=FALSE}
rm(list = ls())
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
p_values <- sapply(trials, function(x) x$p.value)
p_values

p_values <- sapply(trials,`[[`, "p.value")
p_values
```


(2).
```{r,eval=FALSE}
rm(list = ls())
parallel_lapply <- function(X, FUN, ...) {
  result <- Map(FUN, X, ...)
  vapply(result, FUN = identity, FUN.VALUE = numeric(1))
}
inputs <- list(1:5, 6:10)
parallel_lapply(inputs, sum)

```



4.

(1).
```{r,eval=FALSE}
rm(list = ls())
better_chisq <- function(x, y) {
  observed <- table(x, y)
  chisq_stat <- sum((observed - mean(observed))^2 / mean(observed))
  return(chisq_stat)
}
x <- c(1, 2, 1, 2, 1)
y <- c(1, 1, 2, 2, 2)
better_chisq(x, y)
```


(2).
```{r,eval=FALSE}
rm(list = ls())
better_table <- function(x, y) {
  table_result <- table(x, y)
  return(table_result)
}
x <- c(1, 2, 1, 2, 1)
y <- c(1, 1, 2, 2, 2)
better_table(x, y)
observed <- better_table(x, y)
chisq_stat <- sum((observed - mean(observed))^2 / mean(observed))
chisq_stat

```


## Homework-2024.11.18



### Question

This example appears in [40]. Consider the bivariate density
$$
f(x,y) \propto \binom{n}{x} y^{x+a-1} (1-y)^{n-x+b-1},\quad x=0,1,\dots,n, \, 0 \leq y \leq1.
$$
It can be shown (see e.g. [23]) that for fixed $a$,$b$,$n$, the conditional distributions are Binomial($n, y$) and Beta($x + a, n − x + b$). Use the Gibbs sampler to generate a chain with target joint density $f(x, y)$.



### Answer


Clear Variable Space. 

```{r,eval=FALSE}
rm(list=ls())
```



1.


The specific `Rcpp` functions are as follows.  

```{r,eval=FALSE}
cppFunction('
  DataFrame gibbs_sampler_C(int n, double a, double b, int num_samples) {
  NumericVector x(num_samples);
  NumericVector y(num_samples);
  
  int current_x = 0;
  
  for (int i = 0; i < num_samples; i++) {
    y[i] = R::rbeta(current_x + a, n - current_x + b);
    current_x = R::rbinom(n, y[i]);
    x[i] = current_x;
  }
  return DataFrame::create(Named("iteration") = seq(1, num_samples),
                           Named("x") = x,
                           Named("y") = y);
}
')
```




2.

```{r,eval=FALSE}

gibbs_sampler_R <- function(n, a, b, num_samples) {
  samples <- data.frame(iteration = 1:num_samples, x = numeric(num_samples), y = numeric(num_samples))
  x <- 0 
  for (i in 1:num_samples) {
    y <- rbeta(1, x + a, n - x + b)
    x <- rbinom(1, n, y)
    samples[i, "x"] <- x
    samples[i, "y"] <- y
  }
  return(samples)
}

#library(Rcpp)
#dir_cpp <- '../Rcpp/'
#sourceCpp(paste0(dir_cpp,"Gibbs_sampler_C.cpp")) 

n <- 10  
a <- 1   
b <- 1 
num_samples <- 1000 

set.seed(1234)
samples_R <- gibbs_sampler_R(n, a, b, num_samples)
samples_C <- gibbs_sampler_C(n, a, b, num_samples)
samples_long_R <- reshape2::melt(samples_R, id.vars = "iteration")
samples_long_C <- reshape2::melt(samples_C, id.vars = "iteration")

par(mfrow = c(1, 2))
qqplot(samples_R$x, samples_C$x, main = "QQ Plot of x", xlab = "R", ylab = "C", pch = 8, col = "red", cex = 0.8)
abline(0, 1, col = "blue", lwd = 1.8)
qqplot(samples_R$y, samples_C$y, main = "QQ Plot of y", xlab = "R", ylab = "C", pch = 16, col = "red", cex = 0.1)
abline(0, 1, col = "blue", lwd = 1.8)

```

A QQ plot is used to compare the consistency of distributions between two datasets. From the plot, we observe that the data points are close to \( y = x \), indicating that the distributions of the two datasets are very similar. When the curve is slightly above \( y = x \), it suggests that the data generated by C++ is slightly higher in certain regions compared to the data generated by R. This discrepancy might be caused by differences in random number seeds, floating-point arithmetic, or subtle implementation variations.  


Overall, the random number distributions generated by the Gibbs sampling functions implemented in R and C++ are highly consistent.  




3.

```{r,eval=FALSE}
ts <- microbenchmark(
  gibbsR = gibbs_sampler_R(n, a, b, num_samples),
  gibbsC = gibbs_sampler_C(n, a, b, num_samples)
)
summary(ts)[, c(1, 3, 5, 6)]
```


The median runtime of `gibbsR` is 42,223.45 ns, which is significantly higher than the 560.90 ns of `gibbsC`. This means the C++ implementation of Gibbs sampling is approximately 75 times faster than the R implementation. This performance difference arises because C++ executes compiled code with higher efficiency, while R, as an interpreted language, performs slower in loops and computationally intensive tasks.



4.


The `Rcpp` Gibbs sampling function significantly outperforms the R implementation in terms of runtime efficiency. For tasks requiring a large number of iterations (e.g., Markov Chain Monte Carlo) or dealing with large datasets, C++ is the better choice. R’s loops and complex computations are slower. However, the sample distributions generated by both methods are similar, indicating consistent implementation logic. Therefore, it is recommended to use the `Rcpp` implementation in scenarios with high-performance requirements while leveraging R’s ease of use for initial testing and debugging.




